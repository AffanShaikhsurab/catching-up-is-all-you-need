# Research Papers üìö

A curated collection of cutting-edge AI research papers focused on **parameter-efficient fine-tuning**, **large language models**, and **transformer architectures**.

---

## Papers in This Collection

### 1. LoRA Without Regret (Thinking Machines Lab)
**File:** `thinkingmachines-ai-blog-lora-.pdf`

A groundbreaking study demonstrating that **LoRA (Low-Rank Adaptation)** can achieve performance comparable to full fine-tuning when configured correctly, using approximately **67% less computational resources**.

**Key Findings:**
- LoRA performs best when applied to **all weight matrices** (including MLP/MoE layers), not just attention layers
- The optimal learning rate for LoRA is consistently ~10x higher than for full fine-tuning
- LoRA is particularly effective for **reinforcement learning tasks**
- Establishes a "low-regret regime" where LoRA matches full fine-tuning performance

**Authors:** John Schulman et al. (Thinking Machines Lab)  
**Published:** September 2025

---

### 2. EMNLP 2025 Findings Paper
**File:** `2025.findings-emnlp.321.pdf`

A paper from the **EMNLP 2025 Findings** track focusing on natural language processing research. EMNLP (Empirical Methods in Natural Language Processing) is one of the premier venues for NLP research.

---

### 3. arXiv Paper 2411.04965
**File:** `2411.04965v1.pdf`

A research paper from the **arXiv preprint server** (November 2024 submission based on the ID pattern). Papers in this ID range typically cover recent advances in machine learning and AI.

---

### 4. arXiv Paper 2504.15777
**File:** `2504.15777v1.pdf`

A research paper from **April 2025** based on the arXiv ID pattern. This is a more recent submission covering cutting-edge research.

---

### 5. arXiv Paper 2510.04871
**File:** `2510.04871v1.pdf`

A research paper from **October 2025**, representing the latest developments in the field.

---

### 6. arXiv Paper 2511.23404
**File:** `2511.23404v1.pdf`

A research paper from **November 2025** covering cutting-edge AI research. This is one of the larger papers in the collection, suggesting comprehensive research with extensive experiments and analysis.

---

### 7. arXiv Paper 2601.05038
**File:** `2601.05038v1.pdf`

A very recent paper from **January 2026**, capturing the most current research directions.

---

## üåü Cutting-Edge Researchers to Follow

### John Schulman
**Chief Scientist, Thinking Machines Lab**  
üîó [Personal Website](https://joschu.net) | [Google Scholar](https://scholar.google.com/citations?user=jYVDNP4AAAAJ)

- PhD from UC Berkeley (Robotics & RL under Pieter Abbeel)
- Co-founder of OpenAI, led the ChatGPT reinforcement learning team
- Pioneered foundational RL algorithms: **PPO (Proximal Policy Optimization)**, **TRPO**
- Research focus: Reinforcement learning, language model alignment, model transparency

### Mira Murati
**Founder & CEO, Thinking Machines Lab**  
üîó [LinkedIn](https://www.linkedin.com/in/mikimurati/) | [Twitter](https://twitter.com/maborovska)

- Former CTO of OpenAI
- Founded Thinking Machines Lab in February 2025
- Expert in AI systems development and scaling

### Pieter Abbeel
**Professor, UC Berkeley | Co-founder, Covariant**  
üîó [Personal Website](https://people.eecs.berkeley.edu/~pabbeel/) | [Google Scholar](https://scholar.google.com/citations?user=vtwH6GkAAAAJ)

- Leading expert in robotics and reinforcement learning
- John Schulman's PhD advisor
- Research focus: Robot learning, deep RL, imitation learning

---

## Transformer Architecture Pioneers

### Ashish Vaswani
**Co-author of "Attention Is All You Need"**  
üîó [Google Scholar](https://scholar.google.com/citations?user=oR9sCGYAAAAJ)

- Original architect of the transformer model
- Founding research at Google Brain

### Noam Shazeer
**Co-founder, Character.AI**  
üîó [Google Scholar](https://scholar.google.com/citations?user=jVT21XAAAAAJ)

- Co-author of the original transformer paper
- Pioneered many innovations in LLM architecture

### Llion Jones
**Researcher, Sakana AI**  
üîó [Twitter](https://twitter.com/lllooonnnzzz)

- Co-author of original transformer paper
- Currently researching biologically-inspired architectures (Continuous Thought Machine)
- Questioning whether current transformer scaling is the right path

### Illia Polosukhin
**Co-founder, NEAR Protocol**  
üîó [LinkedIn](https://www.linkedin.com/in/illia-polosukhin/)

- Co-author of original transformer paper
- Now building decentralized infrastructure for AI

---

## Leading AI Research Labs & Researchers

### Sebastian Raschka
**AI Educator & Researcher**  
üîó [Website](https://sebastianraschka.com) | [GitHub](https://github.com/rasbt)

- Prolific educator on LLMs and deep learning
- Author of comprehensive LLM tutorials and research

### Andrej Karpathy
**Founder, Eureka Labs**  
üîó [Personal Website](https://karpathy.ai) | [YouTube](https://www.youtube.com/@AndrejKarpathy)

- Former Director of AI at Tesla, founding member of OpenAI
- Known for educational content on deep learning

### Slav Petrov
**Senior Research Scientist, Google**  
üîó [Google Scholar](https://scholar.google.com/citations?user=j3DP8SAAAAAJ)

- Leading work on large language models at Google
- Expert in NLP and machine learning

---

## üè¢ Key Research Organizations

| Organization | Focus | Notable Models |
|-------------|-------|----------------|
| **OpenAI** | Frontier AI research | GPT-4, GPT-5 |
| **Google DeepMind** | Multimodal AI, scaling | Gemini series |
| **Anthropic** | AI safety & alignment | Claude series |
| **Meta AI** | Open research | LLaMA, OPT |
| **Mistral AI** | Efficient LLMs | Mistral 7B, Mixtral |
| **Thinking Machines Lab** | Post-training research | LoRA research |
| **Sakana AI** | Bio-inspired architectures | CTM research |

---

## üìñ How to Use This Repository

1. Each PDF file contains the full paper for study
2. Read the README for quick summaries
3. Follow the researchers on linked platforms for latest updates
4. Check arXiv for the latest versions of papers

---

*Last updated: January 2026*
